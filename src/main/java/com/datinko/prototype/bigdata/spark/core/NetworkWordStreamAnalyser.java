package com.datinko.prototype.bigdata.spark.core;

import java.util.regex.Pattern;

import com.google.common.collect.Lists;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.SparkConf;
import org.apache.spark.SparkContext;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.sql.SQLContext;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.api.java.StorageLevels;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.Time;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;


/**
 * Use DataFrames and SQL to count words in UTF8 encoded, '\n' delimited text received from the
 * network every second.
 *
 * Usage: JavaSqlNetworkWordCount <hostname> <port>
 * <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data.
 *
 * To run this on your local machine, you need to first run a Netcat server
 *    `$ nc -lk 9999`
 * and then run the example
 *     if you are using IntelliJ - just run it this class! :)
 *
 *     if you want to run it from the command line the use this..(I never do tho!)
 *    `$ bin/run-example com.datinko.prototype.bigdata.spark.core.NetworkWordStreamAnalyser localhost 9999`
 */
public class NetworkWordStreamAnalyser {

    private static final Pattern SPACE = Pattern.compile(" ");
    //private static final Logger LOGGER = Logger.getLogger(SimpleJsonAnalyser.class);

    public static void main(String[] args) {

        //this is the url of our spark server...
        String masterName = "local[2]";

        //this is the hostname and port that we are going to get messages sent to us from.
        String hostname = "localhost";
        int port = 9999;

        SparkConf conf = new SparkConf().setAppName("Datinko Data Analysis Prototype - NetworkWordStreamAnalyser");

        if (args.length > 0 && args.length == 2) {
            hostname = args[0];
            port = Integer.parseInt(args[1]);
        } else if(args.length > 0 && args.length<2) {
            System.err.println("Usage: NetworkWordStreamAnalyser <hostname> <port>");
            System.exit(1);
        }
        conf.setMaster(masterName);

        // Create the context with a 1 second batch size
        JavaStreamingContext streamingContext = new JavaStreamingContext(conf, Durations.seconds(1));


        // Create a JavaReceiverInputDStream on target ip:port and count the
        // words in input stream of \n delimited text (eg. generated by 'nc')
        // Note that no duplication in storage level only for running locally.
        // Replication necessary in distributed scenario for fault tolerance.
        JavaReceiverInputDStream<String> lines = streamingContext.socketTextStream(
                hostname, port, StorageLevels.MEMORY_AND_DISK_SER);

        JavaDStream<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
            @Override
            public Iterable<String> call(String x) {
                return Lists.newArrayList(SPACE.split(x));
            }
        });

        //I assume this gets called every second with the RDD containing everything that we recieved in the past second.

        // Convert RDDs of the words DStream to DataFrame and run SQL query
        words.foreachRDD(new Function2<JavaRDD<String>, Time, Void>() {
            @Override
            public Void call(JavaRDD<String> rdd, Time time) {

                SQLContext sqlContext = JavaSqlContextSingleton.getInstance(rdd.context());

                // Convert JavaRDD[String] to JavaRDD[bean class] to DataFrame
                JavaRDD<JavaRecord> rowRDD = rdd.map(new Function<String, JavaRecord>() {
                    public JavaRecord call(String word) {
                        JavaRecord record = new JavaRecord();
                        record.setWord(word);
                        return record;
                    }
                });
                DataFrame wordsDataFrame = sqlContext.createDataFrame(rowRDD, JavaRecord.class);

                // Register as table
                wordsDataFrame.registerTempTable("words");

                // Do word count on table using SQL and print it
                DataFrame wordCountsDataFrame =
                        sqlContext.sql("select word, count(*) as total from words group by word");
                System.out.println("========= " + time + "=========");
                wordCountsDataFrame.show();
                return null;
            }
        });

        streamingContext.start();
        streamingContext.awaitTermination();
    }

    public void setStreamingLogLevels() {
        boolean log4jInitialized = Logger.getRootLogger().getAllAppenders().hasMoreElements();
        if (!log4jInitialized) {
            // We first log something to initialize Spark's default logging, then we override the
            // logging level.
//            logInfo("Setting log level to [WARN] for streaming example." +
//                    " To override add a custom log4j.properties to the classpath.")
            Logger.getRootLogger().setLevel(Level.WARN);
        }
    }
}
